{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f24e1f44-99db-4dd5-b137-09f25857d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e06c950-0e47-463e-855e-3bd7a2703347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Methods \n",
    "\n",
    "# Create a list of URLS we need to fetch\n",
    "today = datetime.now() \n",
    "current_month = today.month \n",
    "current_year = today.year\n",
    "start_url = \"https://mdhopendata.blob.core.windows.net/verkehrsdetektion/\"\n",
    "\n",
    "\n",
    "def get_urls(year=2023):\n",
    "    \"\"\"URL constructor for api.viz.berlin.de traffic data.\n",
    "\n",
    "    Args:\n",
    "        year (int, optional): How far back in time will the URLs / data go. Defaults to 2023.\n",
    "\n",
    "    Returns:\n",
    "        urls_list: List of URLs pointing to gzip csv of traffic data in Berlin\n",
    "    \"\"\"\n",
    "    urls_list =[]\n",
    "    for year in range(year, current_year+1):\n",
    "        #2021 and 2023 have different url patterns\n",
    "        if year == 2021:\n",
    "            mid_url = f\"{year}/Messquerschnitt/mq_hr_{year}_\"\n",
    "        elif year == 2023:\n",
    "            mid_url = f\"{year}/Messquerschnitte%20(fahrtrichtungsbezogen)/mq_hr_{year}_\"\n",
    "        else:\n",
    "            mid_url = f\"{year}/Messquerschnitt%20(fahrtrichtungsbezogen)/mq_hr_{year}_\"\n",
    "        base_url = start_url + mid_url\n",
    "\n",
    "        # Loop through every month of the year\n",
    "        for month in range(1, 13):\n",
    "            # Break if current year, current month is reached\n",
    "            if year == current_year and month == current_month:\n",
    "                break\n",
    "            # Finish URL and append to list\n",
    "            appendix = f\"{month:02d}.csv.gz\"\n",
    "            url = base_url + appendix\n",
    "            urls_list.append(url)\n",
    "    return urls_list\n",
    "\n",
    "def get_traffic_data(year=2023):\n",
    "    urls_list = get_urls(year)\n",
    "    for index, url in enumerate(urls_list):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = io.BytesIO(response.content)\n",
    "            if index == 0:\n",
    "                df = pd.read_csv(data, compression='gzip', delimiter=\";\")\n",
    "            else:\n",
    "                df_next = pd.read_csv(data, compression='gzip', delimiter=\";\")\n",
    "                df = pd.concat([df, df_next])\n",
    "            print(len(df))\n",
    "        except requests.exceptions.HTTPError as errh:\n",
    "            print(f\"{url[-14:]} not found\")\n",
    "    return df\n",
    "\n",
    "def get_sensor_location_data():\n",
    "    \"\"\"Loads the location data of the traffic sensors in Berlin (VIZ Berlin)\n",
    "\n",
    "    Returns:\n",
    "        pandas.Dataframe: _description_\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = \"https://mdhopendata.blob.core.windows.net/verkehrsdetektion/Stammdaten_Verkehrsdetektion_2022_07_20.xlsx\"\n",
    "        response = requests.get(url)\n",
    "        location_data = pd.read_excel(io.BytesIO(response.content))\n",
    "        return location_data\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(f\"Sensor location data not found\")\n",
    "\n",
    "def duplicates_drop(df, name):\n",
    "    \"\"\"Deletes duplicates from dataframe. Prints out how many duplicates were deleted.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe\n",
    "        name (str): Name of the dataframe for printing\n",
    "\n",
    "    Returns:\n",
    "        df (pandas.DataFrame): Cleaned dataframe from duplicates\n",
    "    \"\"\"\n",
    "    before_drop = len(df)\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    after_drop = len(df)\n",
    "\n",
    "    if after_drop < before_drop:\n",
    "        print(f\"{name}: {before_drop - after_drop} duplicates deleted\")\n",
    "    if after_drop == before_drop:\n",
    "        print(f\"{name}: No duplicates\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8175837-3183-423e-ab61-ab4b6e54d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT DATA\n",
    "\n",
    "fact_table = get_traffic_data(2018)\n",
    "location_dim =  get_sensor_location_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a836570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User typed no\n"
     ]
    }
   ],
   "source": [
    "# SAVE RAW DATA (OPTIONAL)\n",
    "\n",
    "yes_choices = ['yes', 'y']\n",
    "no_choices = ['no', 'n']\n",
    "\n",
    "while True:\n",
    "    user_input = input('Do you want rewrite the file?(yes/no): ')\n",
    "\n",
    "    if user_input.lower() in yes_choices:\n",
    "        print('User typed yes')\n",
    "        fact_table.to_csv(\"vehicle_data.csv\", index = False)\n",
    "        break\n",
    "    elif user_input.lower() in no_choices:\n",
    "        print('User typed no')\n",
    "        break\n",
    "    else:\n",
    "        print('Type yes/no')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db752368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 duplicates deleted\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORM DATA \n",
    "\n",
    "## fact_table: Rename the columns according to data model\n",
    "fact_table = fact_table.rename(columns={\"mq_name\":\"mq_name_id\",\"tag\": \"date\", \"stunde\" : \"hour\"})\n",
    "fact_table[\"datetime_id\"] = pd.to_datetime(fact_table[\"date\"] + ' ' + fact_table[\"hour\"].apply(str), format='%d.%m.%Y %H')\n",
    "fact_table.drop([\"date\", \"hour\"], axis=1, inplace=True)\n",
    "fact_table['mq_name_id'] = fact_table['mq_name_id'].map(lambda x: x.rstrip('n'))\n",
    "\n",
    "## location_dim: Rename the columns according to data model\n",
    "location_dim  = location_dim.rename(columns={\"DET_ID15\": \"det_id\", \"MQ_KURZNAME\":\"mq_name_id\",\"LÃ„NGE (WGS84)\": \"lng\" ,\"BREITE (WGS84)\":\"lat\", \"POSITION\": \"desc\"})\n",
    "location_dim = location_dim[[\"det_id\", \"mq_name_id\", \"lat\", \"lng\", \"desc\"]]\n",
    "\n",
    "\n",
    "## Duplication check and drop\n",
    "fact_table = duplicates_drop(fact_table, \"fact_table\")\n",
    "location_dim = duplicates_drop(location_dim, \"location_dim\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc6a27b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Datetime_dim \n",
    "\n",
    "# Datetime dimension table from fact table\n",
    "datetime_dim = fact_table[['datetime_id']].reset_index(drop=True)\n",
    "# Delete all the duplication\n",
    "datetime_dim = datetime_dim.drop_duplicates().reset_index(drop=True)\n",
    "datetime_dim[\"weekday\"] = datetime_dim[\"datetime_id\"].dt.weekday\n",
    "datetime_dim[\"hour\"] = datetime_dim[\"datetime_id\"].dt.hour\n",
    "datetime_dim[\"day\"] = datetime_dim[\"datetime_id\"].dt.day\n",
    "datetime_dim[\"month\"] = datetime_dim[\"datetime_id\"].dt.month\n",
    "datetime_dim[\"year\"] = datetime_dim[\"datetime_id\"].dt.year\n",
    "\n",
    "\n",
    "datetime_dim.to_csv(\"data/datetime_dim.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2031ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE DATA \n",
    "fact_table.to_csv(\"./data/fact_table.csv\", index=False)\n",
    "location_dim.to_csv(\"./data/location_dim.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
